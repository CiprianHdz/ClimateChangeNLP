{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s795J-Guiz3S"
      },
      "outputs": [],
      "source": [
        "# Creating example documents\n",
        "doc_1 = \"A whopping 96.5 percent of water on Earth is in our oceans, covering 71 percent of the surface of our planet. And at any given time, about 0.001 percent is floating above us in the atmosphere. If all of that water fell as rain at once, the whole planet would get about 1 inch of rain.\"\n",
        "doc_2 = \"One-third of your life is spent sleeping. Sleeping 7-9 hours each night should help your body heal itself, activate the immune system, and give your heart a break. Beyond that--sleep experts are still trying to learn more about what happens once we fall asleep.\"\n",
        "doc_3 = \"A newborn baby is 78 percent water. Adults are 55-60 percent water. Water is involved in just about everything our body does.\"\n",
        "doc_4 = \"While still in high school, a student went 264.4 hours without sleep, for which he won first place in the 10th Annual Great San Diego Science Fair in 1964.\"\n",
        "doc_5 = \"We experience water in all three states: solid ice, liquid water, and gas water vapor.\"\n",
        "\n",
        "# Create corpus\n",
        "corpus = [doc_1, doc_2, doc_3, doc_4, doc_5] # List of documents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer # WordNet: english lexic database\n",
        "from gensim import corpora # Gensim: library for topic modeling. Corpora: module to work with corpus\n",
        "from gensim.models import LsiModel # Imports the Latent Semantic Analysis (LSI) class\n",
        "from gensim.models import LdaModel # Imports the Latent Dirichlet Allocation (LDA) class\n",
        "\n",
        "# Remove stopwords, punctuation and normalize the corpus\n",
        "stop = set(stopwords.words('english')) # Creates a set of common words in english without relevance to the text, ie. \"the\", \"is\", \"in\"...\n",
        "exclude = set(string.punctuation) # Creates a set of punctuation marks\n",
        "lemma = WordNetLemmatizer() # Initializes a WordNetLemmatizer object to reduce words to their base or root form (considers context and morphology)\n",
        "\n",
        "def clean(doc):\n",
        "  stop_free = \" \".join([i for i in doc.lower().split() if i not in stop]) # Turns everything to lowercase and separates by words. If it is not a stopword, it saves it as an item in the list\n",
        "  punc_free = \"\".join(ch for ch in stop_free if ch not in exclude) # Removes punctuation marks\n",
        "  normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split()) # Lemmatizes the words\n",
        "  return normalized # Returns a list with cleaned and lemmatized words\n",
        "\n",
        "clean_corpus = [clean(doc).split() for doc in corpus] # Creates a list of documents in the corpus, where each document is a list with cleaned and lemmatized words\n",
        "\n",
        "# Creating document-term matrix\n",
        "dictionary = corpora.Dictionary(clean_corpus) # Creates dictionary: assigns an unique ID to each word\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_corpus] # Converts each document as a list of tuples (word_id, word_count), creating a document-term matrix. doc_term_matrix is a list of BoW representations for each document\n",
        "\n",
        "# LSA model - Reduces dimensionality of data using Singular Value Decomposition (SVD). Similarity between documents is determined by cosine similarity.\n",
        "lsa = LsiModel(doc_term_matrix, num_topics = 3, id2word = dictionary) # Instanciates a LsiModel object with a matrix representation of the document, the number of topics to extract, and a mapping from word IDs to words\n",
        "\n",
        "# LSA model results -> Relationship between words\n",
        "print(lsa.print_topics(num_topics = 3, num_words = 3)) # Prints the 3 most significant topics, each represented as a linear combination of keywords. Each topic has 3 keywords along with their weights\n",
        "\n",
        "# LDA model - Bayesian network. Generative statistical model that maps documents with a list of topics.\n",
        "lda = LdaModel(doc_term_matrix, num_topics=3, id2word=dictionary) # Creates a LdaModel object with the document-term matrix, the number of topics to identify, and mapping from IDs to words\n",
        "\n",
        "# LDA model results -> Topics in corpus of text\n",
        "print(lda.print_topics(num_topics=3, num_words=3)) #Prints the top 3 topics with each represented by the 3 most significant words along with their probabilities"
      ],
      "metadata": {
        "id": "T60d1gEhjbIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a145da2b-66ec-407c-806d-c2a9b03ade74"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '0.555*\"water\" + 0.489*\"percent\" + 0.239*\"rain\"'), (1, '-0.361*\"sleeping\" + -0.215*\"still\" + -0.215*\"hour\"'), (2, '-0.562*\"water\" + 0.231*\"planet\" + 0.231*\"rain\"')]\n",
            "[(0, '0.032*\"percent\" + 0.027*\"planet\" + 0.025*\"rain\"'), (1, '0.040*\"sleeping\" + 0.025*\"still\" + 0.024*\"hour\"'), (2, '0.112*\"water\" + 0.056*\"percent\" + 0.021*\"body\"')]\n"
          ]
        }
      ]
    }
  ]
}