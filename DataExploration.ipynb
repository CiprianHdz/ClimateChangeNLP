{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Data\n",
    "\n",
    "For now, let work with posts, would be nice if you could replicate this notebook with comments. "
   ],
   "id": "2d2fd5d915694867"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data=pd.read_csv('the-reddit-climate-change-dataset-posts.csv')",
   "id": "ed0fa7ae5e6b53a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For now let us only consider the following variables\n",
    "\n",
    "* subreddit.name\n",
    "* created_utc\n",
    "* permalink\n",
    "* domain\n",
    "* url\n",
    "* selftext\n",
    "* title\n",
    "* score\n"
   ],
   "id": "8ada35af21480a51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Task**: \n",
    "\n",
    "Add in this notebook, based on the description of the dataset provided in kaggle, the meaning of each of the variables above.\n",
    "For instance, `score` is the number of votes given to the post and answer: Why do you think we had no use for the rest of them?"
   ],
   "id": "545383e72ad52d17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "slected_variables=[\"subreddit.name\", \"created_utc\", \"permalink\", \"domain\", \"url\", \"selftext\", \"title\", \"score\"]",
   "id": "da24d8573329e663",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data=data[slected_variables].copy()",
   "id": "9fbc84beda1e005f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Task**: \n",
    "\n",
    "\n",
    "Could you add a conclusion based on `data.info()` below ?"
   ],
   "id": "4357061622002af0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data.info()",
   "id": "c13ef664802d149f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here is an example of an entry in our dataset:",
   "id": "224d802cb20225df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data.iloc[0]",
   "id": "e348fe3b9762f688",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Example of a post",
   "id": "5116f56f088222ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data[\"title\"].iloc[0]",
   "id": "a2240c426df3235a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since we are interested in the users behaviour over a certain period of time, sometimes there is a need to create eitherc more informative variables or to transform the ones we already have. This is sometimes refer in ML as *feature engineering*.\n",
    "\n",
    "For example, it isn't strightforward to identy the date with `created_utc` as it is, we must consider a transformation to have a more interpretable representation, in our case is `pd.to_datetime(data['created_utc'], unit='s')` (*Look it up*)"
   ],
   "id": "cdb402c4fc9b82a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data['created_utc'] = pd.to_datetime(data['created_utc'], unit='s')\n",
    "data['year'] = data['created_utc'].dt.year"
   ],
   "id": "824c8d14a6ed1dfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data['year'].unique()",
   "id": "e7a1ac76a65248e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notice we have posts from 2010 to 2022. For fun, let us only consider before and after 2012, due to a rumor the world was to be ended in such year. ",
   "id": "ff679aa45b8ac2bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_bf=data[data['year']<=2012].copy()\n",
    "data_af=data[data['year']>2012].copy()"
   ],
   "id": "9f7f2c1aafd96cbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****",
   "id": "db29a488554593d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploratory Data Analysis",
   "id": "9902aab63817e8e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The function below returns the frequency of each category.  ",
   "id": "483d77c68cf7cc2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def count_categories(categories):\n",
    "    category_counts = {}\n",
    "    for category in categories:\n",
    "        if category in category_counts:\n",
    "            category_counts[category] += 1\n",
    "        else:\n",
    "            category_counts[category] = 1\n",
    "\n",
    "    return list(category_counts.items())"
   ],
   "id": "113963b1808e5aad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categories_count = count_categories(data['subreddit.name']) \n",
    "print(categories_count)"
   ],
   "id": "683b4b7de6d4ab3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets now see if the most frequent categories changed after 2012.",
   "id": "a0aad668b3695627"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categories_count = count_categories(data_bf['subreddit.name']) \n",
    "filtered_categories = list(itertools.filterfalse(lambda x: x[1] <= 100, categories_count))\n",
    "word_freq_dict = dict(filtered_categories)\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq_dict)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Most frequent categories before 2012\")\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  \n",
    "plt.show()"
   ],
   "id": "2ea79ddc016e5dc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categories_count = count_categories(data_af['subreddit.name']) \n",
    "filtered_categories = list(itertools.filterfalse(lambda x: x[1] <= 5000, categories_count))\n",
    "word_freq_dict = dict(filtered_categories)\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq_dict)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Most frequent categories after 2012\")\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  \n",
    "plt.show()"
   ],
   "id": "a492bb47c8a41f88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Task**: \n",
    "\n",
    "Add a conclusion below with the differences (if exists) between before and after 2012 most frequent categories. Is the code the same? what changed?"
   ],
   "id": "1f1d499101b184f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Finding Entities  ",
   "id": "22cfe9376d7522f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For now, lets only use complete data. \n",
    "data = data.dropna(subset=['selftext'])"
   ],
   "id": "a2f05b394177766a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data=data.sample(n=1000).reset_index(drop=True)",
   "id": "2e3ee2c707eb2957",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "nltk.download('stopwords')",
   "id": "3f1e0221613bb3aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Spacy model and stopwords\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))  # Set for faster lookup"
   ],
   "id": "4eaedc5dae833e33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove stopwords from the text\n",
    "data['selftext'] = data['selftext'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))"
   ],
   "id": "fcccd09e712c4c10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to extract locations (GPE entities) from text\n",
    "def extract_locations(text):\n",
    "    return [ent.text for ent in nlp(text).ents if ent.label_ == 'GPE']"
   ],
   "id": "ba170cdcc7569100",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract locations from the 'body' column\n",
    "locations = [loc for sublist in data['selftext'].apply(extract_locations) for loc in sublist]"
   ],
   "id": "8408b1e7607a3168",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get top 50 most common locations\n",
    "most_common_50 = Counter(locations).most_common(50)"
   ],
   "id": "2884a56b3e18f1fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare data for plotting\n",
    "all_locs = [loc for loc, _ in most_common_50]\n",
    "num_loc_mentions = [count * 50 for _, count in most_common_50]\n",
    "avg_loc_sents = [data[data['selftext'].str.contains(loc, regex=False)]['score'].mean() for loc in all_locs]"
   ],
   "id": "b2ff45d960550433",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotting the data\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.scatter(all_locs, avg_loc_sents, s=num_loc_mentions, alpha=0.5)\n",
    "# Add titles and labels\n",
    "plt.title(\"Average Score For Top 50 Mentioned Locations and Number of Mentions in First 1000 Rows\")\n",
    "plt.xlabel(\"Top 50 Mentioned Locations\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Average Sentiment\")\n",
    "plt.legend([\"Bubble Size = Number of Mentions\"])\n",
    "plt.show()\n"
   ],
   "id": "34d41c66cbfcce9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Task**: \n",
    "\n",
    "Do you think this plot is okay as it is? Should we filtered it more? how? Try to refine the above plot using your own logic (have fun with it)."
   ],
   "id": "2c830dba51345fc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#START NAMES\n",
    "def extract_names(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == 'PERSON']"
   ],
   "id": "56d6f842271f753b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "persons = [person for sublist in data['selftext'].apply(extract_names) for person in sublist]\n",
    "most_common_50 = Counter(persons).most_common(50)\n",
    "\n",
    "# Prepare data for plotting\n",
    "all_persons = [person for person, _ in most_common_50]\n",
    "num_person_mentions = [count * 50 for _, count in most_common_50]\n",
    "avg_loc_scores = [data[data['selftext'].str.contains(person, regex=False)]['score'].mean() for person in all_persons]"
   ],
   "id": "8fa0ceaf8f0478ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotting the data\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.scatter(all_persons, avg_loc_scores, s=num_person_mentions, alpha=0.5)\n",
    "# Add titles and labels\n",
    "plt.title(\"Average Score For Top 50 Mentioned persons and Number of Mentions in First 1000 Rows\")\n",
    "plt.xlabel(\"Top 50 Mentioned Locations\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Average Sentiment\")\n",
    "plt.legend([\"Bubble Size = Number of Mentions\"])\n",
    "plt.show()"
   ],
   "id": "d75f14bc1bd9a9ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Task**: \n",
    "\n",
    "Add a conclusion"
   ],
   "id": "31f181ebf0263d73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extra: \n",
    "\n",
    "This is some preview of our future work, but keep them in mind and try to find some other interesting questions or suggest ideas to answer the ones below.\n",
    "\n",
    "Strongly suggest to use chatGPT."
   ],
   "id": "2d31dc23eb47dbab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. What is the number of posts per year?\n",
    "2. Average score (number of votes) per year per category ?\n",
    "3. We want to measure popularity and influence of the found persons.  How can we measure popularity and/or Influence?\n",
    "5. How people are affected and cope with climate change? For example, who engages in these kind of conversations, can we say something about their age or context of the authors of these posts? (This also called author profiling)\n",
    "6. How can we identify false information or fake news with this data?"
   ],
   "id": "7b38415bd4118382"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
